{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a251301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "PAD_ID = 0\n",
    "\n",
    "\n",
    "class DateData:\n",
    "    def __init__(self, n):\n",
    "        np.random.seed(1)\n",
    "        self.date_cn = []\n",
    "        self.date_en = []\n",
    "        for timestamp in np.random.randint(143835585, 2043835585, n):\n",
    "            date = datetime.datetime.fromtimestamp(timestamp)\n",
    "            self.date_cn.append(date.strftime(\"%y-%m-%d\"))\n",
    "            self.date_en.append(date.strftime(\"%d/%b/%Y\"))\n",
    "        self.vocab = set(\n",
    "            [str(i) for i in range(0, 10)] + [\"-\", \"/\", \"<GO>\", \"<EOS>\"] + [\n",
    "                i.split(\"/\")[1] for i in self.date_en])\n",
    "        self.v2i = {v: i for i, v in enumerate(sorted(list(self.vocab)), start=1)}\n",
    "        self.v2i[\"<PAD>\"] = PAD_ID\n",
    "        self.vocab.add(\"<PAD>\")\n",
    "        self.i2v = {i: v for v, i in self.v2i.items()}\n",
    "        self.x, self.y = [], []\n",
    "        for cn, en in zip(self.date_cn, self.date_en):\n",
    "            self.x.append([self.v2i[v] for v in cn])\n",
    "            self.y.append(\n",
    "                [self.v2i[\"<GO>\"], ] + [self.v2i[v] for v in en[:3]] + [\n",
    "                    self.v2i[en[3:6]], ] + [self.v2i[v] for v in en[6:]] + [\n",
    "                    self.v2i[\"<EOS>\"], ])\n",
    "        self.x, self.y = np.array(self.x), np.array(self.y)\n",
    "        self.start_token = self.v2i[\"<GO>\"]\n",
    "        self.end_token = self.v2i[\"<EOS>\"]\n",
    "\n",
    "    def sample(self, n=64):\n",
    "        bi = np.random.randint(0, len(self.x), size=n)\n",
    "        bx, by = self.x[bi], self.y[bi]\n",
    "        decoder_len = np.full((len(bx),), by.shape[1] - 1, dtype=np.int32)\n",
    "        return bx, by, decoder_len\n",
    "\n",
    "    def idx2str(self, idx):\n",
    "        x = []\n",
    "        for i in idx:\n",
    "            x.append(self.i2v[i])\n",
    "            if i == self.end_token:\n",
    "                break\n",
    "        return \"\".join(x)\n",
    "\n",
    "    @property\n",
    "    def num_word(self):\n",
    "        return len(self.vocab)\n",
    "\n",
    "\n",
    "def pad_zero(seqs, max_len):\n",
    "    padded = np.full((len(seqs), max_len), fill_value=PAD_ID, dtype=np.long)\n",
    "    for i, seq in enumerate(seqs):\n",
    "        padded[i, :len(seq)] = seq\n",
    "    return padded\n",
    "\n",
    "\n",
    "def maybe_download_mrpc(save_dir=\"./MRPC/\", proxy=None):\n",
    "    train_url = 'https://mofanpy.com/static/files/MRPC/msr_paraphrase_train.txt'\n",
    "    test_url = 'https://mofanpy.com/static/files/MRPC/msr_paraphrase_test.txt'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    proxies = {\"http\": proxy, \"https\": proxy}\n",
    "    for url in [train_url, test_url]:\n",
    "        raw_path = os.path.join(save_dir, url.split(\"/\")[-1])\n",
    "        if not os.path.isfile(raw_path):\n",
    "            print(\"downloading from %s\" % url)\n",
    "            r = requests.get(url, proxies=proxies)\n",
    "            with open(raw_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(r.text.replace('\"', \"<QUOTE>\"))\n",
    "                print(\"completed\")\n",
    "\n",
    "\n",
    "def _text_standardize(text):\n",
    "    text = re.sub(r'—', '-', text)\n",
    "    text = re.sub(r'–', '-', text)\n",
    "    text = re.sub(r'―', '-', text)\n",
    "    text = re.sub(r\" \\d+(,\\d+)?(\\.\\d+)? \", \" <NUM> \", text)\n",
    "    text = re.sub(r\" \\d+-+?\\d*\", \" <NUM>-\", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def _process_mrpc(dir=\"./MRPC\", rows=None):\n",
    "    data = {\"train\": None, \"test\": None}\n",
    "    files = os.listdir(dir)\n",
    "    for f in files:\n",
    "        df = pd.read_csv(os.path.join(dir, f), sep='\\t', nrows=rows)\n",
    "        k = \"train\" if \"train\" in f else \"test\"\n",
    "        data[k] = {\"is_same\": df.iloc[:, 0].values, \"s1\": df[\"#1 String\"].values, \"s2\": df[\"#2 String\"].values}\n",
    "    vocab = set()\n",
    "    for n in [\"train\", \"test\"]:\n",
    "        for m in [\"s1\", \"s2\"]:\n",
    "            for i in range(len(data[n][m])):\n",
    "                data[n][m][i] = _text_standardize(data[n][m][i].lower())\n",
    "                cs = data[n][m][i].split(\" \")\n",
    "                vocab.update(set(cs))\n",
    "    v2i = {v: i for i, v in enumerate(sorted(vocab), start=1)}\n",
    "    v2i[\"<PAD>\"] = PAD_ID\n",
    "    v2i[\"<MASK>\"] = len(v2i)\n",
    "    v2i[\"<SEP>\"] = len(v2i)\n",
    "    v2i[\"<GO>\"] = len(v2i)\n",
    "    i2v = {i: v for v, i in v2i.items()}\n",
    "    for n in [\"train\", \"test\"]:\n",
    "        for m in [\"s1\", \"s2\"]:\n",
    "            data[n][m+\"id\"] = [[v2i[v] for v in c.split(\" \")] for c in data[n][m]]\n",
    "    return data, v2i, i2v\n",
    "\n",
    "\n",
    "class MRPCData:\n",
    "    num_seg = 3\n",
    "    pad_id = PAD_ID\n",
    "\n",
    "    def __init__(self, data_dir=\"./MRPC/\", rows=None, proxy=None):\n",
    "        maybe_download_mrpc(save_dir=data_dir, proxy=proxy)\n",
    "        data, self.v2i, self.i2v = _process_mrpc(data_dir, rows)\n",
    "        self.max_len = max(\n",
    "            [len(s1) + len(s2) + 3 for s1, s2 in zip(\n",
    "                data[\"train\"][\"s1id\"] + data[\"test\"][\"s1id\"], data[\"train\"][\"s2id\"] + data[\"test\"][\"s2id\"])])\n",
    "\n",
    "        self.xlen = np.array([\n",
    "            [\n",
    "                len(data[\"train\"][\"s1id\"][i]), len(data[\"train\"][\"s2id\"][i])\n",
    "             ] for i in range(len(data[\"train\"][\"s1id\"]))], dtype=int)\n",
    "        x = [\n",
    "            [self.v2i[\"<GO>\"]] + data[\"train\"][\"s1id\"][i] + [self.v2i[\"<SEP>\"]] + data[\"train\"][\"s2id\"][i] + [self.v2i[\"<SEP>\"]]\n",
    "            for i in range(len(self.xlen))\n",
    "        ]\n",
    "        self.x = pad_zero(x, max_len=self.max_len)\n",
    "        self.nsp_y = data[\"train\"][\"is_same\"][:, None]\n",
    "\n",
    "        self.seg = np.full(self.x.shape, self.num_seg-1, np.int32)\n",
    "        for i in range(len(x)):\n",
    "            si = self.xlen[i][0] + 2\n",
    "            self.seg[i, :si] = 0\n",
    "            si_ = si + self.xlen[i][1] + 1\n",
    "            self.seg[i, si:si_] = 1\n",
    "\n",
    "        self.word_ids = np.array(list(set(self.i2v.keys()).difference(\n",
    "            [self.v2i[v] for v in [\"<PAD>\", \"<MASK>\", \"<SEP>\"]])))\n",
    "\n",
    "    def sample(self, n):\n",
    "        bi = np.random.randint(0, self.x.shape[0], size=n)\n",
    "        bx, bs, bl, by = self.x[bi], self.seg[bi], self.xlen[bi], self.nsp_y[bi]\n",
    "        return bx, bs, bl, by\n",
    "\n",
    "    @property\n",
    "    def num_word(self):\n",
    "        return len(self.v2i)\n",
    "\n",
    "    @property\n",
    "    def mask_id(self):\n",
    "        return self.v2i[\"<MASK>\"]\n",
    "\n",
    "\n",
    "class MRPCSingle:\n",
    "    pad_id = PAD_ID\n",
    "\n",
    "    def __init__(self, data_dir=\"./MRPC/\", rows=None, proxy=None):\n",
    "        maybe_download_mrpc(save_dir=data_dir, proxy=proxy)\n",
    "        data, self.v2i, self.i2v = _process_mrpc(data_dir, rows)\n",
    "\n",
    "        self.max_len = max([len(s) + 2 for s in data[\"train\"][\"s1id\"] + data[\"train\"][\"s2id\"]])\n",
    "        x = [\n",
    "            [self.v2i[\"<GO>\"]] + data[\"train\"][\"s1id\"][i] + [self.v2i[\"<SEP>\"]]\n",
    "            for i in range(len(data[\"train\"][\"s1id\"]))\n",
    "        ]\n",
    "        x += [\n",
    "            [self.v2i[\"<GO>\"]] + data[\"train\"][\"s2id\"][i] + [self.v2i[\"<SEP>\"]]\n",
    "            for i in range(len(data[\"train\"][\"s2id\"]))\n",
    "        ]\n",
    "        self.x = pad_zero(x, max_len=self.max_len)\n",
    "        self.word_ids = np.array(list(set(self.i2v.keys()).difference([self.v2i[\"<PAD>\"]])))\n",
    "\n",
    "    def sample(self, n):\n",
    "        bi = np.random.randint(0, self.x.shape[0], size=n)\n",
    "        bx = self.x[bi]\n",
    "        return bx\n",
    "\n",
    "    @property\n",
    "    def num_word(self):\n",
    "        return len(self.v2i)\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, x, y, v2i, i2v):\n",
    "        self.x, self.y = x, y\n",
    "        self.v2i, self.i2v = v2i, i2v\n",
    "        self.vocab = v2i.keys()\n",
    "\n",
    "    def sample(self, n):\n",
    "        b_idx = np.random.randint(0, len(self.x), n)\n",
    "        bx, by = self.x[b_idx], self.y[b_idx]\n",
    "        return bx, by\n",
    "\n",
    "    @property\n",
    "    def num_word(self):\n",
    "        return len(self.v2i)\n",
    "\n",
    "\n",
    "def process_w2v_data(corpus, skip_window=2, method=\"skip_gram\"):\n",
    "    all_words = [sentence.split(\" \") for sentence in corpus]\n",
    "    all_words = np.array(list(itertools.chain(*all_words)))\n",
    "    # vocab sort by decreasing frequency for the negative sampling below (nce_loss).\n",
    "    vocab, v_count = np.unique(all_words, return_counts=True)\n",
    "    vocab = vocab[np.argsort(v_count)[::-1]]\n",
    "\n",
    "    print(\"all vocabularies sorted from more frequent to less frequent:\\n\", vocab)\n",
    "    v2i = {v: i for i, v in enumerate(vocab)}\n",
    "    i2v = {i: v for v, i in v2i.items()}\n",
    "\n",
    "    # pair data\n",
    "    pairs = []\n",
    "    js = [i for i in range(-skip_window, skip_window + 1) if i != 0]\n",
    "\n",
    "    for c in corpus:\n",
    "        words = c.split(\" \")\n",
    "        w_idx = [v2i[w] for w in words]\n",
    "        if method == \"skip_gram\":\n",
    "            for i in range(len(w_idx)):\n",
    "                for j in js:\n",
    "                    if i + j < 0 or i + j >= len(w_idx):\n",
    "                        continue\n",
    "                    pairs.append((w_idx[i], w_idx[i + j]))  # (center, context) or (feature, target)\n",
    "        elif method.lower() == \"cbow\":\n",
    "            for i in range(skip_window, len(w_idx) - skip_window):\n",
    "                context = []\n",
    "                for j in js:\n",
    "                    context.append(w_idx[i + j])\n",
    "                pairs.append(context + [w_idx[i]])  # (contexts, center) or (feature, target)\n",
    "        else:\n",
    "            raise ValueError\n",
    "    pairs = np.array(pairs)\n",
    "    print(\"5 example pairs:\\n\", pairs[:5])\n",
    "    if method.lower() == \"skip_gram\":\n",
    "        x, y = pairs[:, 0], pairs[:, 1]\n",
    "    elif method.lower() == \"cbow\":\n",
    "        x, y = pairs[:, :-1], pairs[:, -1]\n",
    "    else:\n",
    "        raise ValueError\n",
    "    return Dataset(x, y, v2i, i2v)\n",
    "\n",
    "\n",
    "def set_soft_gpu(soft_gpu):\n",
    "    import tensorflow as tf\n",
    "    if soft_gpu:\n",
    "        gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            # Currently, memory growth needs to be the same across GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
