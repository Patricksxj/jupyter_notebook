{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T03:10:02.814645Z",
     "start_time": "2020-02-22T03:10:02.619634Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.graphx.{Graph, GraphLoader, VertexId, VertexRDD}\r\n",
       "import org.apache.spark.rdd.RDD\r\n",
       "import org.apache.spark.{SparkConf, SparkContext}\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.graphx.{Graph, GraphLoader, VertexId, VertexRDD}\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.{SparkConf, SparkContext}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T03:10:52.888149Z",
     "start_time": "2020-02-22T03:10:52.820546Z"
    }
   },
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "13: error: not found: type VertexRDD\r",
     "output_type": "error",
     "traceback": [
      "<console>:13: error: not found: type VertexRDD\r",
      "         val vertices: VertexRDD[VD]\r",
      "                       ^",
      "<console>:14: error: not found: type EdgeRDD\r",
      "         val edges: EdgeRDD[ED]}\r",
      "                    ^",
      ""
     ]
    }
   ],
   "source": [
    "class Graph[VD, ED] {\n",
    "  val vertices: VertexRDD[VD]\n",
    "  val edges: EdgeRDD[ED]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T03:10:55.584493Z",
     "start_time": "2020-02-22T03:10:55.514890Z"
    }
   },
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "26: error: class $iw needs to be abstract, since value userGraph is not defined\r",
     "output_type": "error",
     "traceback": [
      "<console>:26: error: class $iw needs to be abstract, since value userGraph is not defined\r",
      "class $iw extends Serializable {\r",
      "      ^",
      ""
     ]
    }
   ],
   "source": [
    "val userGraph: Graph[(String, String), String]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T03:11:02.410931Z",
     "start_time": "2020-02-22T03:11:01.189313Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "users: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, (String, String))] = ParallelCollectionRDD[0] at parallelize at <console>:31\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val users: RDD[(VertexId, (String, String))] = \n",
    "    sc.parallelize(Array((3L, (\"rxin\", \"student\")),\n",
    "                        (7L, (\"jgonzal\", \"postdoc\")),\n",
    "                        (5L, (\"franklin\", \"prof\")),\n",
    "                        (2L, (\"istoica\", \"prof\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T03:11:08.815572Z",
     "start_time": "2020-02-22T03:11:08.693565Z"
    }
   },
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "30: error: not found: type Edge\r",
     "output_type": "error",
     "traceback": [
      "<console>:30: error: not found: type Edge\r",
      "       val relationships: RDD[Edge[String]] = sc.parallelize(Array(\r",
      "                              ^",
      "<console>:31: error: not found: value Edge\r",
      "Error occurred in an application involving default arguments.\r",
      "           Edge(3L, 7L, \"collab\"),\r",
      "           ^",
      "<console>:32: error: not found: value Edge\r",
      "Error occurred in an application involving default arguments.\r",
      "           Edge(5L, 3L, \"advisor\"),\r",
      "           ^",
      "<console>:33: error: not found: value Edge\r",
      "Error occurred in an application involving default arguments.\r",
      "           Edge(2L, 5L, \"colleague\"),\r",
      "           ^",
      "<console>:34: error: not found: value Edge\r",
      "Error occurred in an application involving default arguments.\r",
      "           Edge(5L, 7L, \"pi\")\r",
      "           ^",
      ""
     ]
    }
   ],
   "source": [
    "val relationships: RDD[Edge[String]] = sc.parallelize(Array(\n",
    "    Edge(3L, 7L, \"collab\"),\n",
    "    Edge(5L, 3L, \"advisor\"),\n",
    "    Edge(2L, 5L, \"colleague\"),\n",
    "    Edge(5L, 7L, \"pi\")\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T03:06:41.788074Z",
     "start_time": "2020-02-22T03:06:41.724070Z"
    }
   },
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "27: error: not found: value Graph\r",
     "output_type": "error",
     "traceback": [
      "<console>:27: error: not found: value Graph\r",
      "       val graph = Graph(users, relationships, defaultUser)\r",
      "                   ^",
      "<console>:27: error: not found: value users\r",
      "       val graph = Graph(users, relationships, defaultUser)\r",
      "                         ^",
      "<console>:27: error: not found: value relationships\r",
      "       val graph = Graph(users, relationships, defaultUser)\r",
      "                                ^",
      ""
     ]
    }
   ],
   "source": [
    "val defaultUser = (\"John Doe\", \"Missing\")\n",
    "val graph = Graph(users, relationships, defaultUser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T09:54:16.661354Z",
     "start_time": "2020-01-05T09:54:13.014145Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Long = 1\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.vertices.filter {case (id, (name, pos)) => pos == \"postdoc\"}.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T09:56:11.538925Z",
     "start_time": "2020-01-05T09:56:11.158903Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: Array[(org.apache.spark.graphx.VertexId, (String, String))] = Array((7,(jgonzal,postdoc)))\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.vertices.filter {case (id, (name, pos)) => pos == \"postdoc\"}.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T09:54:33.369310Z",
     "start_time": "2020-01-05T09:54:32.933285Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Long = 2\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.vertices.filter {case (id, (name, pos)) => pos == \"prof\"}.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.edges.filter(e => e.srcId < e.dstId).count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T09:56:39.859544Z",
     "start_time": "2020-01-05T09:56:39.478523Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: Array[org.apache.spark.graphx.Edge[String]] = Array(Edge(3,7,collab), Edge(2,5,colleague), Edge(5,7,pi))\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.edges.filter(e => e.srcId < e.dstId).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T10:13:20.234763Z",
     "start_time": "2020-01-05T10:13:19.767736Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: Long = 4\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//边数\n",
    "graph.numEdges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T09:31:13.389235Z",
     "start_time": "2020-01-05T09:31:11.205110Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "users: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, (String, String))] = ParallelCollectionRDD[0] at parallelize at <console>:43\r\n",
       "relationships: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[String]] = ParallelCollectionRDD[1] at parallelize at <console>:47\r\n",
       "defaultUser: (String, String) = (John Doe,Missing)\r\n",
       "graph: org.apache.spark.graphx.Graph[(String, String),String] = org.apache.spark.graphx.impl.GraphImpl@6a94c96\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Assume the SparkContext has already been constructed\n",
    "//val sc: SparkContext\n",
    "// Create an RDD for the vertices\n",
    "val users: RDD[(VertexId, (String, String))] =\n",
    "  sc.parallelize(Array((3L, (\"rxin\", \"student\")), (7L, (\"jgonzal\", \"postdoc\")),\n",
    "                       (5L, (\"franklin\", \"prof\")), (2L, (\"istoica\", \"prof\"))))\n",
    "// Create an RDD for edges\n",
    "val relationships: RDD[Edge[String]] =\n",
    "  sc.parallelize(Array(Edge(3L, 7L, \"collab\"),    Edge(5L, 3L, \"advisor\"),\n",
    "                       Edge(2L, 5L, \"colleague\"), Edge(5L, 7L, \"pi\")))\n",
    "// Define a default user in case there are relationship with missing user\n",
    "val defaultUser = (\"John Doe\", \"Missing\")\n",
    "// Build the initial Graph\n",
    "val graph = Graph(users, relationships, defaultUser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T09:32:07.056305Z",
     "start_time": "2020-01-05T09:32:06.629281Z"
    }
   },
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "37: error: class $iw needs to be abstract, since value graph is not defined\r",
     "output_type": "error",
     "traceback": [
      "<console>:37: error: class $iw needs to be abstract, since value graph is not defined\r",
      "class $iw extends Serializable {\r",
      "      ^",
      ""
     ]
    }
   ],
   "source": [
    "val graph: Graph[(String, String), String] // Constructed from above\n",
    "// Count all users which are postdocs\n",
    "graph.vertices.filter { case (id, (name, pos)) => pos == \"postdoc\" }.count\n",
    "// Count all the edges where src > dst\n",
    "graph.edges.filter(e => e.srcId > e.dstId).count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T09:41:03.135967Z",
     "start_time": "2020-01-05T09:41:03.076964Z"
    }
   },
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "13: error: not found: type VertexRDD\r",
     "output_type": "error",
     "traceback": [
      "<console>:13: error: not found: type VertexRDD\r",
      "         val vertices: VertexRDD[VD]\r",
      "                       ^",
      "<console>:14: error: not found: type EdgeRDD\r",
      "         val edges: EdgeRDD[ED]\r",
      "                    ^",
      ""
     ]
    }
   ],
   "source": [
    "class Graph[VD, ED] {\n",
    "  val vertices: VertexRDD[VD]\n",
    "  val edges: EdgeRDD[ED]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T09:24:26.889985Z",
     "start_time": "2020-01-05T09:24:26.779979Z"
    }
   },
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "28: error: class $iw needs to be abstract, since value userGraph is not defined\r",
     "output_type": "error",
     "traceback": [
      "<console>:28: error: class $iw needs to be abstract, since value userGraph is not defined\r",
      "class $iw extends Serializable {\r",
      "      ^",
      ""
     ]
    }
   ],
   "source": [
    "val userGraph: Graph[(String, String), String]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T09:25:35.756924Z",
     "start_time": "2020-01-05T09:25:35.692920Z"
    }
   },
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "16: error: not found: type Graph\r",
     "output_type": "error",
     "traceback": [
      "<console>:16: error: not found: type Graph\r",
      "       var graph: Graph[VertexProperty, String] = null\r",
      "                  ^",
      ""
     ]
    }
   ],
   "source": [
    "class VertexProperty()\n",
    "case class UserProperty(val name: String) extends VertexProperty\n",
    "case class ProductProperty(val name: String, val price: Double) extends VertexProperty\n",
    "//The graph might then have the type:\n",
    "var graph: Graph[VertexProperty, String] = null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T09:35:41.662580Z",
     "start_time": "2020-01-05T09:35:41.565574Z"
    }
   },
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "44: error: not found: type VD\r",
     "output_type": "error",
     "traceback": [
      "<console>:44: error: not found: type VD\r",
      "         val vertices: VertexRDD[VD]\r",
      "                                 ^",
      "<console>:57: error: not found: type ED\r",
      "         val edges: EdgeRDD[ED]\r",
      "                            ^",
      "<console>:75: error: not found: type VD\r",
      "         val triplets: RDD[EdgeTriplet[VD, ED]]\r",
      "                                       ^",
      "<console>:75: error: not found: type ED\r",
      "         val triplets: RDD[EdgeTriplet[VD, ED]]\r",
      "                                           ^",
      ""
     ]
    }
   ],
   "source": [
    "/**\n",
    "   * An RDD containing the vertices and their associated attributes.\n",
    "   *\n",
    "   * @note vertex ids are unique.\n",
    "   * @return an RDD containing the vertices in this graph\n",
    "   */\n",
    "  val vertices: VertexRDD[VD]\n",
    "\n",
    "  /**\n",
    "   * An RDD containing the edges and their associated attributes.  The entries in the RDD contain\n",
    "   * just the source id and target id along with the edge data.\n",
    "   *\n",
    "   * @return an RDD containing the edges in this graph\n",
    "   *\n",
    "   * @see `Edge` for the edge type.\n",
    "   * @see `Graph#triplets` to get an RDD which contains all the edges\n",
    "   * along with their vertex data.\n",
    "   *\n",
    "   */\n",
    "  val edges: EdgeRDD[ED]\n",
    "\n",
    "  /**\n",
    "   * An RDD containing the edge triplets, which are edges along with the vertex data associated with\n",
    "   * the adjacent vertices. The caller should use [[edges]] if the vertex data are not needed, i.e.\n",
    "   * if only the edge data and adjacent vertex ids are needed.\n",
    "   *\n",
    "   * @return an RDD containing edge triplets\n",
    "   *\n",
    "   * @example This operation might be used to evaluate a graph\n",
    "   * coloring where we would like to check that both vertices are a\n",
    "   * different color.\n",
    "   * {{{\n",
    "   * type Color = Int\n",
    "   * val graph: Graph[Color, Int] = GraphLoader.edgeListFile(\"hdfs://file.tsv\")\n",
    "   * val numInvalid = graph.triplets.map(e => if (e.src.data == e.dst.data) 1 else 0).sum\n",
    "   * }}}\n",
    "   */\n",
    "  val triplets: RDD[EdgeTriplet[VD, ED]]\n",
    "\n",
    "  /**\n",
    "   * Caches the vertices and edges associated with this graph at the specified storage level,\n",
    "   * ignoring any target storage levels previously set.\n",
    "   *\n",
    "   * @param newLevel the level at which to cache the graph.\n",
    "   *\n",
    "   * @return A reference to this graph for convenience.\n",
    "   */"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T10:09:07.581312Z",
     "start_time": "2020-01-05T10:09:06.397244Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.collection.mutable\r\n",
       "import org.apache.spark._\r\n",
       "import org.apache.spark.graphx._\r\n",
       "import org.apache.spark.graphx.PartitionStrategy._\r\n",
       "import org.apache.spark.graphx.lib._\r\n",
       "import org.apache.spark.storage.StorageLevel\r\n",
       "defined object Analytics\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.collection.mutable\n",
    "\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.graphx.PartitionStrategy._\n",
    "import org.apache.spark.graphx.lib._\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "\n",
    "\n",
    "/**\n",
    " * Driver program for running graph algorithms.\n",
    " */\n",
    "object Analytics {\n",
    "\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    if (args.length < 2) {\n",
    "      val usage = \"\"\"Usage: Analytics <taskType> <file> --numEPart=<num_edge_partitions>\n",
    "      |[other options] Supported 'taskType' as follows:\n",
    "      |pagerank    Compute PageRank\n",
    "      |cc          Compute the connected components of vertices\n",
    "      |triangles   Count the number of triangles\"\"\".stripMargin\n",
    "      System.err.println(usage)\n",
    "      System.exit(1)\n",
    "    }\n",
    "\n",
    "    val taskType = args(0)\n",
    "    val fname = args(1)\n",
    "    val optionsList = args.drop(2).map { arg =>\n",
    "      arg.dropWhile(_ == '-').split('=') match {\n",
    "        case Array(opt, v) => (opt -> v)\n",
    "        case _ => throw new IllegalArgumentException(s\"Invalid argument: $arg\")\n",
    "      }\n",
    "    }\n",
    "    val options = mutable.Map(optionsList: _*)\n",
    "\n",
    "    val conf = new SparkConf()\n",
    "    GraphXUtils.registerKryoClasses(conf)\n",
    "\n",
    "    val numEPart = options.remove(\"numEPart\").map(_.toInt).getOrElse {\n",
    "      println(\"Set the number of edge partitions using --numEPart.\")\n",
    "      sys.exit(1)\n",
    "    }\n",
    "    val partitionStrategy: Option[PartitionStrategy] = options.remove(\"partStrategy\")\n",
    "      .map(PartitionStrategy.fromString(_))\n",
    "    val edgeStorageLevel = options.remove(\"edgeStorageLevel\")\n",
    "      .map(StorageLevel.fromString(_)).getOrElse(StorageLevel.MEMORY_ONLY)\n",
    "    val vertexStorageLevel = options.remove(\"vertexStorageLevel\")\n",
    "      .map(StorageLevel.fromString(_)).getOrElse(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "    taskType match {\n",
    "      case \"pagerank\" =>\n",
    "        val tol = options.remove(\"tol\").map(_.toFloat).getOrElse(0.001F)\n",
    "        val outFname = options.remove(\"output\").getOrElse(\"\")\n",
    "        val numIterOpt = options.remove(\"numIter\").map(_.toInt)\n",
    "\n",
    "        options.foreach {\n",
    "          case (opt, _) => throw new IllegalArgumentException(s\"Invalid option: $opt\")\n",
    "        }\n",
    "\n",
    "        println(\"======================================\")\n",
    "        println(\"|             PageRank               |\")\n",
    "        println(\"======================================\")\n",
    "\n",
    "        val sc = new SparkContext(conf.setAppName(s\"PageRank($fname)\"))\n",
    "\n",
    "        val unpartitionedGraph = GraphLoader.edgeListFile(sc, fname,\n",
    "          numEdgePartitions = numEPart,\n",
    "          edgeStorageLevel = edgeStorageLevel,\n",
    "          vertexStorageLevel = vertexStorageLevel).cache()\n",
    "        val graph = partitionStrategy.foldLeft(unpartitionedGraph)(_.partitionBy(_))\n",
    "\n",
    "        println(s\"GRAPHX: Number of vertices ${graph.vertices.count}\")\n",
    "        println(s\"GRAPHX: Number of edges ${graph.edges.count}\")\n",
    "\n",
    "        val pr = (numIterOpt match {\n",
    "          case Some(numIter) => PageRank.run(graph, numIter)\n",
    "          case None => PageRank.runUntilConvergence(graph, tol)\n",
    "        }).vertices.cache()\n",
    "\n",
    "        println(s\"GRAPHX: Total rank: ${pr.map(_._2).reduce(_ + _)}\")\n",
    "\n",
    "        if (!outFname.isEmpty) {\n",
    "          println(s\"Saving pageranks of pages to $outFname\")\n",
    "          pr.map { case (id, r) => id + \"\\t\" + r }.saveAsTextFile(outFname)\n",
    "        }\n",
    "\n",
    "        sc.stop()\n",
    "\n",
    "      case \"cc\" =>\n",
    "        options.foreach {\n",
    "          case (opt, _) => throw new IllegalArgumentException(s\"Invalid option: $opt\")\n",
    "        }\n",
    "\n",
    "        println(\"======================================\")\n",
    "        println(\"|      Connected Components          |\")\n",
    "        println(\"======================================\")\n",
    "\n",
    "        val sc = new SparkContext(conf.setAppName(s\"ConnectedComponents($fname)\"))\n",
    "        val unpartitionedGraph = GraphLoader.edgeListFile(sc, fname,\n",
    "          numEdgePartitions = numEPart,\n",
    "          edgeStorageLevel = edgeStorageLevel,\n",
    "          vertexStorageLevel = vertexStorageLevel).cache()\n",
    "        val graph = partitionStrategy.foldLeft(unpartitionedGraph)(_.partitionBy(_))\n",
    "\n",
    "        val cc = ConnectedComponents.run(graph)\n",
    "        println(s\"Components: ${cc.vertices.map { case (vid, data) => data }.distinct()}\")\n",
    "        sc.stop()\n",
    "\n",
    "      case \"triangles\" =>\n",
    "        options.foreach {\n",
    "          case (opt, _) => throw new IllegalArgumentException(s\"Invalid option: $opt\")\n",
    "        }\n",
    "\n",
    "        println(\"======================================\")\n",
    "        println(\"|      Triangle Count                |\")\n",
    "        println(\"======================================\")\n",
    "\n",
    "        val sc = new SparkContext(conf.setAppName(s\"TriangleCount($fname)\"))\n",
    "        val graph = GraphLoader.edgeListFile(sc, fname,\n",
    "          canonicalOrientation = true,\n",
    "          numEdgePartitions = numEPart,\n",
    "          edgeStorageLevel = edgeStorageLevel,\n",
    "          vertexStorageLevel = vertexStorageLevel)\n",
    "          // TriangleCount requires the graph to be partitioned\n",
    "          .partitionBy(partitionStrategy.getOrElse(RandomVertexCut)).cache()\n",
    "        val triangles = TriangleCount.run(graph)\n",
    "        val triangleTypes = triangles.vertices.map {\n",
    "          case (vid, data) => data.toLong\n",
    "        }.reduce(_ + _) / 3\n",
    "\n",
    "        println(s\"Triangles: ${triangleTypes}\")\n",
    "        sc.stop()\n",
    "\n",
    "      case _ =>\n",
    "        println(\"Invalid task type.\")\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
