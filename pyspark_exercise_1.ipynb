{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.arange(18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext as sc\n",
    "from pyspark import SparkConf\n",
    "conf=SparkConf().setAppName(\"miniProject\").setMaster(\"local[*]\")\n",
    "sc=SparkContext.getOrCreate(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(r'D:\\P_WORKPLACE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\P_WORKPLACE'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    pyspark.stop()\n",
    "except:\n",
    "    pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(list(range(10**2)))\n",
    "print(rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext as sc\n",
    "from pyspark import SparkConf\n",
    "conf=SparkConf().setAppName(\"miniProject\").setMaster(\"local[*]\")\n",
    "sc=SparkContext.getOrCreate(conf)\n",
    "\n",
    "def isprime(n):\n",
    "\n",
    "\n",
    "#check if integer n is a prime\n",
    "\n",
    "\n",
    "# make sure n is a positive integer\n",
    "    n = abs(int(n))\n",
    "# 0 and 1 are not primes\n",
    "    if n < 2:\n",
    "        return False\n",
    "# 2 is the only even prime number\n",
    "    if n == 2:\n",
    "        return True\n",
    "# all other even numbers are not primes\n",
    "    if not n & 1:\n",
    "        return False\n",
    "\n",
    "print(isprime(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "df=sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(r'D:\\P_WORKPLACE\\departuredelays.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(date=1011245, delay=6, distance=602, origin='ABE', destination='ATL')\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unexpected item type: <class 'slice'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-65ecc9b4949e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\SPARK\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1287\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1289\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"unexpected item type: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1291\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unexpected item type: <class 'slice'>"
     ]
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1. filter过滤函数\n",
    "\n",
    "df.filter(df.列名==值) 或者 df.filter(\"列名==值\")   中间可以使用(或操作|)   (与操作&)\n",
    "\n",
    "2. union函数\n",
    "\n",
    "df.union(df1) 可进行行合并的操作，df与df1的列名相同\n",
    "\n",
    "3. 本地集合求并集\n",
    "\n",
    "totalset = set1 | set2   set1=set(pandas['列名'])\n",
    "\n",
    "4. pandas df 选择数据\n",
    "\n",
    "df.iat[i,0]  选择行为i，列为0的数据\n",
    "\n",
    "5. pyspark df选择列\n",
    "\n",
    "df.select(\"列名1\",\"列名2\")\n",
    "\n",
    "6. 获取系统时间\n",
    "\n",
    "import datetime\n",
    "\n",
    "datetime.datetime.now()\n",
    "\n",
    "7. 筛选包含在一特定list中的数据框中的列数据 \n",
    "\n",
    "df.filter(df.列名.isin(list)) 使用isin函数\n",
    "\n",
    "8. 修改列名\n",
    "\n",
    "df.selectExpr(\"列名1 as 新列名1\",\"列名2 as 新列名2\") \n",
    "\n",
    "9. 修改列名第二种方法\n",
    "\n",
    "df.select(col(\"列名1\").alias(\"新列名1\"),col(\"列名2\").alias(\"新列名2\"))\n",
    "\n",
    "10. 数据去重\n",
    "\n",
    "df.dropDuplicates()\n",
    "\n",
    "11. 列合并\n",
    "\n",
    "df.withColumn(\"新列名\",df1.列名)\n",
    "\n",
    "12. 对列数据进行序号化处理\n",
    "\n",
    "df.select(\"列名\").distinct().withColumn(\"新列名\",row_number.over(Window.orderBy(\"列名\"))-1) 对列数据进行升序排序后按照(行号-1)进行重新编号\n",
    "\n",
    "13. 创建临时表进行查询\n",
    "\n",
    "df.createOrReplaceTempView(\"表名\")\n",
    "\n",
    "sql =''' select * from 表名'''\n",
    "\n",
    "sqlContext.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
